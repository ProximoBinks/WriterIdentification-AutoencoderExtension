{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOICGEz6jN_e"
   },
   "source": [
    "# IAM Writer Recognition with an Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the notebook is to use the method explained in the paper [DeepWriter: A Multi-Stream Deep CNN for Text-independent Writer Identification](https://arxiv.org/abs/1606.06472) to identify the writer (author) of a text based on their writing styles. To do so, we'll use the [IAM Handwriting Database](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database/download-the-iam-handwriting-database). Please make sure the dataset has been correctly set up before executing the notebook as outlined [here](https://github.com/diegocasmo/iam_writer_recognition/tree/master/data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inxlDgefjN_j"
   },
   "source": [
    "# Reading The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a dictionary which will map each form ID (sentence) to a writer. This information is available in the ``forms.txt`` file, where each line (except for the first 16 lines, which are documentation) defines the form ID at index ``0``, and its writer at index ``1``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zHuGCQFjN_l"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store each form ID and its writer\n",
    "import os\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from itertools import islice\n",
    "\n",
    "form_writer = {}\n",
    "forms_file_path = \"../data/forms.txt\"\n",
    "with open(forms_file_path) as f:\n",
    "    for line in islice(f, 16, None):\n",
    "        line_list = line.split(' ')\n",
    "        form_id = line_list[0]\n",
    "        writer = line_list[1]\n",
    "        form_writer[form_id] = writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGIUdr5cjN_p"
   },
   "source": [
    "Visualise dictionary (as array for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "FKJ6dsAGjN_q",
    "outputId": "cb98eba3-137f-4573-cd9f-6d172ba7d6cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of form-writer pairs: 1539\n",
      "[('a01-000u', '000'), ('a01-000x', '001'), ('a01-003', '002'), ('a01-003u', '000'), ('a01-003x', '003')]\n",
      "Sample form-writer mappings: [('a01-000u', '000'), ('a01-000x', '001'), ('a01-003', '002'), ('a01-003u', '000'), ('a01-003x', '003')]\n"
     ]
    }
   ],
   "source": [
    "list(form_writer.items())[0:5]\n",
    "print(\"Number of form-writer pairs:\", len(form_writer))\n",
    "print(list(form_writer.items())[0:5])\n",
    "print(\"Sample form-writer mappings:\", list(form_writer.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdHVHlU_jN_t"
   },
   "source": [
    "For efficiency reasons,  we'll select the 50 most common writers from the dictionary we have created, and the rest of the notebook will only focus on them (as opposed to using the 221 authors present in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4le_mL3jN_t"
   },
   "outputs": [],
   "source": [
    "# Select the 50 most common writer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "top_writers = []\n",
    "num_writers = 50\n",
    "writers_counter = Counter(form_writer.values())\n",
    "for writer_id,_ in writers_counter.most_common(num_writers):\n",
    "    top_writers.append(writer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7DKQ0VijN_w"
   },
   "source": [
    "Visualise the writer id of the top 50 writers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5mhwA0LrjN_x",
    "outputId": "7c141aa2-d171-499f-b1fd-738c182cdc83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top writer IDs: ['000', '150', '151', '152', '153']\n",
      "['000', '150', '151', '152', '153']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top writer IDs:\", top_writers[0:5])\n",
    "print(top_writers[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwW7ts-mjN_0"
   },
   "source": [
    "From the 50 most common writers we have selected, we'll now need to select the forms (sentences) they have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeRZBANbjN_0"
   },
   "outputs": [],
   "source": [
    "top_forms = []\n",
    "for form_id, author_id in form_writer.items():\n",
    "    if author_id in top_writers:\n",
    "        top_forms.append(form_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HDd4y12jN_4"
   },
   "source": [
    "Visualise the form id of the top 50 writers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uu5W_aXUjN_6",
    "outputId": "321e8045-0560-454a-80fe-479272f9f369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of top forms: 452\n",
      "Sample form IDs: ['a01-000u', 'a01-003u', 'a01-007u', 'a01-011u', 'a01-014u']\n",
      "['a01-000u', 'a01-003u', 'a01-007u', 'a01-011u', 'a01-014u']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of top forms:\", len(top_forms))\n",
    "print(\"Sample form IDs:\", top_forms[:5])\n",
    "print(top_forms[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be_0siyXjN_-"
   },
   "source": [
    "Create a temp directory which contains only the sentences of the forms selected above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQBL7IeajN__"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Forms: ['a01-000u', 'a01-003u', 'a01-007u', 'a01-011u', 'a01-014u', 'a01-020u', 'a01-026u', 'a01-030u', 'a01-043u', 'a01-049u', 'a01-049x', 'a01-053u', 'a01-058u', 'a01-063u', 'a01-068u', 'a01-072u', 'a01-077u', 'a01-082u', 'a01-087u', 'a01-091u', 'a01-096u', 'a01-102u', 'a01-107u', 'a01-113u', 'a01-117u', 'a01-122u', 'a01-128u', 'a01-132u', 'a01-132x', 'a02-017', 'a02-020', 'a02-024', 'a02-027', 'a02-032', 'a02-037', 'a02-042', 'a02-090', 'a02-093', 'a02-098', 'a02-102', 'a02-106', 'a02-111', 'a02-124', 'a03-047', 'a03-050', 'a03-071', 'a03-073', 'a03-080', 'a03-089', 'a05-000', 'a05-013', 'a05-017', 'a05-022', 'a05-025', 'a05-029', 'a05-039', 'a05-044', 'a05-048', 'a05-053', 'a05-058', 'a05-062', 'a05-069', 'a05-073', 'a05-080', 'a05-084', 'a05-089', 'a05-094', 'a05-099', 'a05-104', 'a05-108', 'a05-113', 'a05-116', 'a05-121', 'a05-125', 'a06-124', 'a06-134', 'a06-141', 'a06-147', 'a06-157', 'b05-055', 'b05-058', 'b05-062', 'b05-067', 'b05-071', 'b06-000', 'b06-008', 'b06-012', 'b06-019', 'b06-023', 'b06-032', 'b06-036', 'b06-056', 'b06-059', 'b06-064', 'b06-071', 'b06-079', 'b06-093', 'b06-097', 'b06-100', 'b06-110', 'c03-000a', 'c03-000b', 'c03-000c', 'c03-000d', 'c03-000e', 'c03-000f', 'c03-003a', 'c03-003b', 'c03-003c', 'c03-003d', 'c03-003e', 'c03-003f', 'c03-007a', 'c03-007b', 'c03-007c', 'c03-007d', 'c03-007e', 'c03-007f', 'c03-016a', 'c03-016b', 'c03-016c', 'c03-016d', 'c03-016e', 'c03-021a', 'c03-021b', 'c03-021c', 'c03-021d', 'c03-021e', 'c03-021f', 'c03-081a', 'c03-081b', 'c03-081c', 'c03-081d', 'c03-081e', 'c03-081f', 'c03-084a', 'c03-084b', 'c03-084c', 'c03-084d', 'c03-084e', 'c03-084f', 'c03-087a', 'c03-087b', 'c03-087c', 'c03-087d', 'c03-087e', 'c03-087f', 'c03-094a', 'c03-094b', 'c03-094c', 'c03-094d', 'c03-094e', 'c03-094f', 'c03-096a', 'c03-096b', 'c03-096c', 'c03-096d', 'c03-096e', 'c03-096f', 'c06-000', 'c06-005', 'c06-011', 'c06-020', 'c06-027', 'c06-031', 'c06-039', 'c06-043', 'c06-052', 'c06-076', 'c06-080', 'c06-083', 'c06-087', 'c06-100', 'c06-116', 'c06-128', 'd06-008', 'd06-015', 'd06-020', 'd06-030', 'd06-046', 'd06-050', 'd06-063', 'd06-082', 'd07-082', 'd07-085', 'd07-089', 'd07-093', 'd07-096', 'd07-100', 'd07-102', 'e07-000', 'g03-049', 'g05-098', 'g06-011a', 'g06-011b', 'g06-011c', 'g06-011e', 'g06-011f', 'g06-011g', 'g06-011h', 'g06-011i', 'g06-011j', 'g06-011k', 'g06-011l', 'g06-011m', 'g06-011n', 'g06-011o', 'g06-011p', 'g06-011r', 'g06-018a', 'g06-018b', 'g06-018c', 'g06-018d', 'g06-018e', 'g06-018f', 'g06-018g', 'g06-018h', 'g06-018i', 'g06-018j', 'g06-018k', 'g06-018l', 'g06-018m', 'g06-018n', 'g06-018o', 'g06-018p', 'g06-018r', 'g06-026a', 'g06-026b', 'g06-026c', 'g06-026d', 'g06-026e', 'g06-026f', 'g06-026g', 'g06-026h', 'g06-026i', 'g06-026j', 'g06-026k', 'g06-026l', 'g06-026m', 'g06-026n', 'g06-026o', 'g06-026p', 'g06-026r', 'g06-031a', 'g06-031b', 'g06-031c', 'g06-031d', 'g06-031e', 'g06-031f', 'g06-031g', 'g06-031h', 'g06-031i', 'g06-031j', 'g06-031k', 'g06-031l', 'g06-031m', 'g06-031n', 'g06-031o', 'g06-031p', 'g06-031r', 'g06-037b', 'g06-037c', 'g06-037d', 'g06-037e', 'g06-037f', 'g06-037g', 'g06-037h', 'g06-037i', 'g06-037j', 'g06-037k', 'g06-037l', 'g06-037m', 'g06-037n', 'g06-037o', 'g06-037p', 'g06-037r', 'g06-042a', 'g06-042b', 'g06-042c', 'g06-042d', 'g06-042e', 'g06-042f', 'g06-042g', 'g06-042h', 'g06-042i', 'g06-042j', 'g06-042k', 'g06-042l', 'g06-042m', 'g06-042n', 'g06-042o', 'g06-042p', 'g06-042r', 'g06-045a', 'g06-045b', 'g06-045c', 'g06-045d', 'g06-045e', 'g06-045f', 'g06-045g', 'g06-045h', 'g06-045i', 'g06-045j', 'g06-045k', 'g06-045l', 'g06-045m', 'g06-045n', 'g06-045o', 'g06-045p', 'g06-045r', 'g06-047a', 'g06-047b', 'g06-047c', 'g06-047d', 'g06-047e', 'g06-047f', 'g06-047g', 'g06-047h', 'g06-047i', 'g06-047j', 'g06-047k', 'g06-047l', 'g06-047m', 'g06-047n', 'g06-047o', 'g06-047p', 'g06-047r', 'g06-050a', 'g06-050b', 'g06-050c', 'g06-050d', 'g06-050e', 'g06-050f', 'g06-050g', 'g06-050h', 'g06-050i', 'g06-050j', 'g06-050k', 'g06-050l', 'g06-050m', 'g06-050n', 'g06-050o', 'g06-050p', 'g06-050r', 'g06-089', 'g06-093', 'g06-096', 'g06-101', 'g06-105', 'g06-109', 'g06-115', 'h05-012', 'h06-000', 'h06-003', 'h06-079', 'h06-082', 'h06-085', 'h06-089', 'h06-092', 'h06-096', 'j06-000', 'j06-005', 'j06-008', 'j06-014', 'j06-018', 'j06-022', 'j06-026', 'j06-030', 'j06-034', 'j06-051', 'j06-056', 'm06-019', 'm06-031', 'm06-042', 'm06-048', 'm06-056', 'm06-067', 'm06-076', 'm06-083', 'm06-091', 'm06-098', 'm06-106', 'n02-098', 'n02-104', 'n02-109', 'n02-114', 'n02-120', 'n02-127', 'n06-074', 'n06-082', 'n06-092', 'n06-100', 'n06-111', 'n06-119', 'n06-123', 'n06-128', 'n06-133', 'n06-140', 'n06-148', 'n06-156', 'n06-163', 'n06-169', 'n06-175', 'n06-182', 'n06-186', 'n06-194', 'n06-201', 'p03-057', 'p03-087', 'p03-096', 'p03-103', 'p03-112', 'p06-030', 'p06-042', 'p06-047', 'p06-052', 'p06-058', 'p06-069', 'p06-088', 'p06-096', 'p06-104', 'p06-242', 'p06-248', 'r03-053', 'r06-000', 'r06-003', 'r06-007', 'r06-011', 'r06-018', 'r06-022', 'r06-027', 'r06-035', 'r06-041', 'r06-044', 'r06-049', 'r06-053', 'r06-057', 'r06-062', 'r06-066', 'r06-070', 'r06-076', 'r06-090', 'r06-097', 'r06-103', 'r06-106', 'r06-111', 'r06-115', 'r06-121', 'r06-126', 'r06-130', 'r06-137', 'r06-143']\n",
      "Files found: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Create temp directory to save writers' forms in (assumes files have already been copied if the directory exists)\n",
    "temp_sentences_path = \"../data/temp_sentences\"\n",
    "if not os.path.exists(temp_sentences_path):\n",
    "    os.makedirs(temp_sentences_path)\n",
    "\n",
    "# Debugging Line 4: Check if 'top_forms' is correctly set\n",
    "print(f\"Top Forms: {top_forms}\")\n",
    "\n",
    "original_sentences_path = os.path.join(\"..\", \"data\", \"sentences\", \"*\", \"*\", \"*.png\")\n",
    "\n",
    "# Debugging Line 5: Verify the Paths\n",
    "print(\"Files found:\", glob.glob(original_sentences_path)[:5])\n",
    "\n",
    "for file_path in glob.glob(original_sentences_path):\n",
    "    image_name = file_path.split(os.path.sep)[-1]  # Use os.path.sep for cross-platform compatibility\n",
    "    form_id = image_name.split('-')[0] + '-' + image_name.split('-')[1]\n",
    "\n",
    "    if form_id in top_forms:\n",
    "        # Debugging Line 6: Check if Files are Copied\n",
    "        print(f\"Copying file {file_path} to {temp_sentences_path}/{image_name}\")\n",
    "        try:\n",
    "            shutil.copy(file_path, os.path.join(temp_sentences_path, image_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to copy {file_path}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSSAlzWVjOAD"
   },
   "source": [
    "Create arrays of file inputs (a form) and their respective targets (a writer id):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfzwf2BKjOAH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array lengths: 0 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "img_files = np.zeros((0), dtype=str)\n",
    "img_targets = []\n",
    "\n",
    "path_to_files = os.path.join(temp_sentences_path, '*')\n",
    "for file_path in glob.glob(path_to_files):\n",
    "    img_files = np.append(img_files, file_path)\n",
    "    file_name, _ = os.path.splitext(file_path.split(os.path.sep)[-1])\n",
    "    form_id = '-'.join(file_name.split('-')[0:2])\n",
    "    if form_id in form_writer:\n",
    "        img_targets.append(form_writer[form_id])\n",
    "\n",
    "# Convert img_targets to a NumPy array\n",
    "img_targets = np.array(img_targets)\n",
    "\n",
    "# Debugging Line 7: Validate Array Populations\n",
    "print(\"Array lengths:\", len(img_files), len(img_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb6AwFvIjOAP"
   },
   "source": [
    "Visualize the form -> writer id arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "KI9InV7sjOAQ",
    "outputId": "42ad7fa7-b577-480a-c626-6cf0fe8b33f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: ../data/temp_sentences\\*\n",
      "Found 0 files.\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Checking path: {path_to_files}\")\n",
    "files_found = glob.glob(path_to_files)\n",
    "print(f\"Found {len(files_found)} files.\")\n",
    "\n",
    "print(img_files[0:5])\n",
    "print(img_targets[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKiDRakVjOAS"
   },
   "source": [
    "Visualize dataset's images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "J1m2JtZyjOAT",
    "outputId": "65742ceb-8bbb-489a-846e-c07deaea435d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "for file_name in img_files[:2]:\n",
    "    img = mpimg.imread(file_name)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(img, cmap ='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yYh5_3vjOAV"
   },
   "source": [
    "Encode writers with a value between 0 and ``n_classes-1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "y9A4WbqPjOAV",
    "outputId": "d1f8fcc5-8127-4231-b7cc-690c10dea186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writer ID        :  []\n",
      "Encoded writer ID:  []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_img_targets = encoder.fit_transform(img_targets)\n",
    "\n",
    "print(\"Writer ID        : \", img_targets[:2])\n",
    "print(\"Encoded writer ID: \", encoded_img_targets[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPMqJtNnjOAZ"
   },
   "source": [
    "Split dataset into train, validation, and tests sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SqFgVvjEjOAZ",
    "outputId": "5d069de7-a41a-4180-8106-42d213f4cd69",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Split dataset into training and test sets\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_img_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Further split training set into training and validation sets\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2433\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2430\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2432\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2433\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2111\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2108\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2115\u001b[0m     )\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(img_files, encoded_img_targets, test_size=0.2, shuffle = True)\n",
    "\n",
    "# Further split training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle = True)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5-nRNWFjOAc"
   },
   "source": [
    "Define a couple of constants that will be used throughout the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UXwDegTjOAd"
   },
   "outputs": [],
   "source": [
    "CROP_SIZE = 113\n",
    "NUM_LABELS = 50\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested in the paper, the input to the model are not unique sentences but rather random patches cropped from each sentence. The ``get_augmented_sample`` method is in charge of doing so by resizing each sentence's height to ``113`` pixels, and its width such that original aspect ratio is maintained. Finally, from the resized image, patches of ``113x113`` are randomly cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckp7A0ZbjOAi"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def get_augmented_sample(sample, label, sample_ratio):\n",
    "    # Get current image details\n",
    "    img = Image.open(sample)\n",
    "    img_width = img.size[0]\n",
    "    img_height = img.size[1]\n",
    "\n",
    "    # Compute resize dimensions such that aspect ratio is maintained\n",
    "    height_fac = CROP_SIZE / img_height\n",
    "    size = (int(img_width * height_fac), CROP_SIZE)\n",
    "\n",
    "    # Resize image \n",
    "    new_img = img.resize((size), Image.ANTIALIAS)\n",
    "    new_img_width = new_img.size[0]\n",
    "    new_img_height = new_img.size[1]\n",
    "\n",
    "    # Generate a random number of crops of size 113x113 from the resized image\n",
    "    x_coord = list(range(0, new_img_width - CROP_SIZE))\n",
    "    num_crops = int(len(x_coord) * sample_ratio)\n",
    "    random_x_coord = random.sample(x_coord, num_crops)\n",
    "    \n",
    "    # Create augmented images (cropped forms) and map them to a label (writer)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for x in random_x_coord:\n",
    "        img_crop = new_img.crop((x, 0, x + CROP_SIZE, CROP_SIZE))\n",
    "        # Transform image to an array of numbers\n",
    "        images.append(np.asarray(img_crop))\n",
    "        labels.append(label)\n",
    "\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what the ``get_augmented_sample`` method does by augmenting one sample from the training set. Let's first take a look at how the original image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, label = X_train[0], y_train[0]\n",
    "img = mpimg.imread(sample)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(img, cmap ='gray')\n",
    "print(\"Label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A now, let's augment it and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_augmented_sample(sample, label, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``labels`` returned by the ``get_augmented_sample`` is simply the label of the original image for each cropped patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(\"Num of labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ``images`` returned by it are the random patches created from the original image (only two samples shown for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(images))\n",
    "plt.imshow(images[0], cmap ='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[1], cmap ='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses a generator in order to be able to call ``get_augmented_sample`` when training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QpwonMROjOAm",
    "outputId": "66054b7c-046c-447d-c284-90ead6a2d4d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def generate_data(samples, labels, batch_size, sample_ratio):\n",
    "    while 1: \n",
    "        for offset in range(0, len(samples), batch_size):\n",
    "            batch_samples = samples[offset:(offset + batch_size)]\n",
    "            batch_labels = labels[offset:(offset + batch_size)]\n",
    "            \n",
    "            # Augment each sample in batch\n",
    "            augmented_batch_samples = []\n",
    "            augmented_batch_labels = []\n",
    "            for i in range(len(batch_samples)):\n",
    "                sample = batch_samples[i]\n",
    "                label = batch_labels[i]\n",
    "                augmented_samples, augmented_labels = get_augmented_sample(sample, label, sample_ratio)\n",
    "                augmented_batch_samples.append(augmented_samples)\n",
    "                augmented_batch_labels.append(augmented_labels)\n",
    "\n",
    "            # Flatten out samples and labels\n",
    "            augmented_batch_samples = reduce(operator.add, augmented_batch_samples)\n",
    "            augmented_batch_labels = reduce(operator.add, augmented_batch_labels)\n",
    "            \n",
    "            # Reshape input format\n",
    "            X_train = np.array(augmented_batch_samples)\n",
    "            X_train = X_train.reshape(X_train.shape[0], CROP_SIZE, CROP_SIZE, 1)\n",
    "\n",
    "            # Transform input to float and normalize\n",
    "            X_train = X_train.astype('float32')\n",
    "            X_train /= 255\n",
    "\n",
    "            # Encode y\n",
    "            y_train = np.array(augmented_batch_labels)\n",
    "            y_train = to_categorical(y_train, NUM_LABELS)\n",
    "\n",
    "            yield X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeHRbGh_jOAo"
   },
   "source": [
    "Create training, validation, and test generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9dr2pogjOAp"
   },
   "outputs": [],
   "source": [
    "train_generator = generate_data(X_train, y_train, BATCH_SIZE, 0.3)\n",
    "validation_generator = generate_data(X_val, y_val, BATCH_SIZE, 0.3)\n",
    "test_generator = generate_data(X_test, y_test, BATCH_SIZE, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KeHK6NkTjOAr",
    "outputId": "cccba7d3-41fe-4924-8259-57d209c216d9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fvu-0oV4jOAu"
   },
   "outputs": [],
   "source": [
    "def resize_image(img):\n",
    "    size = round(CROP_SIZE/2)\n",
    "    return tf.image.resize(img, [size, size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used is exactly the same as the one in the \"[handwriting_recognition](https://github.com/priya-dwivedi/Deep-Learning/tree/master/handwriting_recognition)\" notebook by [Priyanka Dwivedi](https://github.com/priya-dwivedi):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "mR01wBGijOAx",
    "outputId": "b8f4c4d1-6599-4569-d08e-2d48a060f47a"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Define network input shape\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(CROP_SIZE, CROP_SIZE, 1)))\n",
    "# Resize images to allow for easy computation\n",
    "model.add(Lambda(resize_image))\n",
    "\n",
    "# CNN model - Building the model suggested in paper\n",
    "model.add(Convolution2D(filters= 32, kernel_size =(5,5), strides= (2, 2), padding='same', name='conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "model.add(Convolution2D(filters= 64, kernel_size =(3, 3), strides= (1, 1), padding='same', name='conv2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "model.add(Convolution2D(filters= 128, kernel_size =(3, 3), strides= (1, 1), padding='same', name='conv3'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, name='dense1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, name='dense2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(NUM_LABELS, name='output'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the model is trained for 20 epochs and the models obtained after each epoch are saved to the ``./model_checkpoints`` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1397
    },
    "colab_type": "code",
    "id": "AWcVj_1WjOAy",
    "outputId": "5fb3ea7c-b304-40f3-ad21-ab2b06aad5a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create directory to save checkpoints at\n",
    "model_checkpoints_path = \"./model_checkpoints\"\n",
    "if not os.path.exists(model_checkpoints_path):\n",
    "    os.makedirs(model_checkpoints_path)\n",
    "    \n",
    "# Save model after every epoch using checkpoints\n",
    "create_checkpoint = ModelCheckpoint(\n",
    "    filepath = \"./model_checkpoints/check_{epoch:02d}_{val_loss:.4f}.hdf5\",\n",
    "    verbose = 1,\n",
    "    save_best_only = False\n",
    ")\n",
    "\n",
    "# Fit model using generators\n",
    "history_object = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch = round(len(X_train) / BATCH_SIZE),\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = round(len(X_val) / BATCH_SIZE),\n",
    "    epochs = 20,\n",
    "    verbose = 1,\n",
    "    callbacks = [create_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wec61jeMjOA1"
   },
   "source": [
    "Load a saved model weights and use them to predict labels in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K-VNI527jOA1",
    "outputId": "0e05eb13-7c63-43b5-c433-4523e6ce28b3"
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"./model_checkpoints/model_weights.hdf5\"\n",
    "if model_weights_path:\n",
    "    model.load_weights(model_weights_path)\n",
    "    scores = model.evaluate_generator(test_generator, steps=round(len(X_test)/BATCH_SIZE))\n",
    "    print(\"Accuracy: \", scores[1])\n",
    "else:\n",
    "    print(\"Set model weights file to load in the 'model_weights_path' variable\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "solution.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
