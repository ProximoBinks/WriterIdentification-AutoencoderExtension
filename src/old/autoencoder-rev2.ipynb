{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd6b1a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The dataset is empty. Please check your data loading and preprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 225>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Main code\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     X_train, y_train, X_val, y_val, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# Create and train the autoencoder\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     autoencoder \u001b[38;5;241m=\u001b[39m create_autoencoder()\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(img_data)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_samples \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset is empty. Please check your data loading and preprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Split the dataset into training, validation, and test sets\u001b[39;00m\n\u001b[0;32m    200\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(img_data, img_targets_encoded, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The dataset is empty. Please check your data loading and preprocessing."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Lambda, Activation\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "# Constants\n",
    "CROP_SIZE = 113\n",
    "NUM_LABELS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Function to create the autoencoder\n",
    "def create_autoencoder():\n",
    "    input_img = Input(shape=(CROP_SIZE, CROP_SIZE, 1))\n",
    "    encoded = Dense(128, activation='relu')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(1, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Function to extract features using the autoencoder\n",
    "def extract_features(encoder_model, data):\n",
    "    data_features = encoder_model.predict(data.reshape((len(data), CROP_SIZE * CROP_SIZE)))\n",
    "    return data_features\n",
    "\n",
    "# Function to create and compile the writer identification model\n",
    "def create_writer_identification_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define network input shape\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(CROP_SIZE, CROP_SIZE, 1)))\n",
    "    model.add(Lambda(resize_image))\n",
    "\n",
    "    # CNN model\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name='conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', name='conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', name='conv3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(512, name='dense1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(256, name='dense2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(NUM_LABELS, name='output'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to generate data batches\n",
    "def generate_data(samples, labels, batch_size, sample_ratio):\n",
    "    while 1:\n",
    "        for offset in range(0, len(samples), batch_size):\n",
    "            batch_samples = samples[offset:(offset + batch_size)]\n",
    "            batch_labels = labels[offset:(offset + batch_size)]\n",
    "\n",
    "            # Augment each sample in batch\n",
    "            augmented_batch_samples = []\n",
    "            augmented_batch_labels = []\n",
    "            for i in range(len(batch_samples)):\n",
    "                sample = batch_samples[i]\n",
    "                label = batch_labels[i]\n",
    "                augmented_samples, augmented_labels = get_augmented_sample(sample, label, sample_ratio)\n",
    "                augmented_batch_samples.append(augmented_samples)\n",
    "                augmented_batch_labels.append(augmented_labels)\n",
    "\n",
    "            # Flatten out samples and labels\n",
    "            augmented_batch_samples = reduce(operator.add, augmented_batch_samples)\n",
    "            augmented_batch_labels = reduce(operator.add, augmented_batch_labels)\n",
    "\n",
    "            # Reshape input format\n",
    "            X_train = np.array(augmented_batch_samples)\n",
    "            X_train = X_train.reshape(X_train.shape[0], CROP_SIZE, CROP_SIZE, 1)\n",
    "\n",
    "            # Transform input to float and normalize\n",
    "            X_train = X_train.astype('float32')\n",
    "            X_train /= 255\n",
    "\n",
    "            # Encode y\n",
    "            y_train = np.array(augmented_batch_labels)\n",
    "            y_train = to_categorical(y_train, NUM_LABELS)\n",
    "\n",
    "            yield X_train, y_train\n",
    "\n",
    "# Function to resize images\n",
    "def resize_image(img):\n",
    "    size = round(CROP_SIZE / 2)\n",
    "    return tf.image.resize(img, [size, size])\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_and_preprocess_data():\n",
    "    # Create a dictionary to store each form ID and its writer\n",
    "    form_writer = {}\n",
    "    forms_file_path = \"../data/forms.txt\"\n",
    "    with open(forms_file_path) as f:\n",
    "        for line in islice(f, 16, None):\n",
    "            line_list = line.split(' ')\n",
    "            form_id = line_list[0]\n",
    "            writer = line_list[1]\n",
    "            form_writer[form_id] = writer\n",
    "\n",
    "    # Select the 50 most common writers\n",
    "    top_writers = []\n",
    "    num_writers = 50\n",
    "    writers_counter = Counter(form_writer.values())\n",
    "    for writer_id, _ in writers_counter.most_common(num_writers):\n",
    "        top_writers.append(writer_id)\n",
    "\n",
    "    # Select the forms (sentences) written by the top 50 writers\n",
    "    top_forms = []\n",
    "    for form_id, author_id in form_writer.items():\n",
    "        if author_id in top_writers:\n",
    "            top_forms.append(form_id)\n",
    "\n",
    "    # Create a temp directory containing only the selected sentences\n",
    "    temp_sentences_path = \"../data/temp_sentences\"\n",
    "    if not os.path.exists(temp_sentences_path):\n",
    "        os.makedirs(temp_sentences_path)\n",
    "\n",
    "    original_sentences_path = os.path.join(\"../data/sentences\", \"*\", \"*\", \"*.png\")\n",
    "\n",
    "    for file_path in glob.glob(original_sentences_path):\n",
    "        image_name = file_path.split(os.path.sep)[-1]\n",
    "        form_id = image_name.split('-')[0] + '-' + image_name.split('-')[1]\n",
    "\n",
    "        if form_id in top_forms:\n",
    "            try:\n",
    "                shutil.copy(file_path, os.path.join(temp_sentences_path, image_name))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to copy {file_path}. Error: {e}\")\n",
    "\n",
    "    # Create arrays of file inputs (a form) and their respective targets (a writer id)\n",
    "    img_files = np.zeros((0), dtype=str)\n",
    "    img_targets = []\n",
    "\n",
    "    path_to_files = os.path.join(temp_sentences_path, \"*\", \"*\", \"*.png\")\n",
    "    for file_path in glob.glob(path_to_files):\n",
    "        img_files = np.append(img_files, file_path)\n",
    "        img_targets.append(form_writer[file_path.split(os.path.sep)[-2]])\n",
    "\n",
    "    # Encode target values\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(img_targets)\n",
    "    img_targets_encoded = encoder.transform(img_targets)\n",
    "\n",
    "    # Load images\n",
    "    img_data = []\n",
    "    for i, img_path in enumerate(img_files):\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_data.append(np.array(img))\n",
    "        print(f\"Loaded image {i + 1}/{len(img_files)}: {img_path}\")\n",
    "\n",
    "    # Normalize the pixel values\n",
    "    img_data = np.array(img_data)\n",
    "    img_data = img_data / 255.0\n",
    "\n",
    "    # Check the number of samples in your dataset\n",
    "    num_samples = len(img_data)\n",
    "    if num_samples == 0:\n",
    "        raise ValueError(\"The dataset is empty. Please check your data loading and preprocessing.\")\n",
    "\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(img_data, img_targets_encoded, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Function to train the writer identification model\n",
    "def train_writer_identification_model(X_train, y_train, X_val, y_val):\n",
    "    writer_identification_model = create_writer_identification_model()\n",
    "\n",
    "    # Define model checkpoint to save the best model\n",
    "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    # Train the model\n",
    "    history = writer_identification_model.fit(\n",
    "        generate_data(X_train, y_train, BATCH_SIZE, sample_ratio=0.5),\n",
    "        steps_per_epoch=len(X_train) / BATCH_SIZE,\n",
    "        validation_data=(X_val, to_categorical(y_val, NUM_LABELS)),\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return writer_identification_model, history\n",
    "\n",
    "# Main code\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder = create_autoencoder()\n",
    "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Extract features using the autoencoder\n",
    "    encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[3].output)\n",
    "    X_train_features = extract_features(encoder_model, X_train)\n",
    "    X_val_features = extract_features(encoder_model, X_val)\n",
    "    X_test_features = extract_features(encoder_model, X_test)\n",
    "\n",
    "    # Train the writer identification model\n",
    "    writer_identification_model, history = train_writer_identification_model(X_train_features, y_train, X_val_features, y_val)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = writer_identification_model.evaluate(X_test_features, to_categorical(y_test, NUM_LABELS))\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd52bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ee5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
