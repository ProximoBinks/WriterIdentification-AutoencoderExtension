{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac03969",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1a3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Lambda, Activation\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "# Constants\n",
    "CROP_SIZE = 113\n",
    "NUM_LABELS = 50\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b44a8",
   "metadata": {},
   "source": [
    "## Create Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd31e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the autoencoder\n",
    "def create_autoencoder():\n",
    "    input_img = Input(shape=(CROP_SIZE, CROP_SIZE, 1))\n",
    "    encoded = Dense(128, activation='relu')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(1, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdebe4c2",
   "metadata": {},
   "source": [
    "## Extract Features Using Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c6c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features using the autoencoder\n",
    "def extract_features(encoder_model, data):\n",
    "    data_features = encoder_model.predict(data.reshape((len(data), CROP_SIZE * CROP_SIZE)))\n",
    "    return data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e722da",
   "metadata": {},
   "source": [
    "## Create Writer Identification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec289de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and compile the writer identification model\n",
    "def create_writer_identification_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define network input shape\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(CROP_SIZE, CROP_SIZE, 1)))\n",
    "    model.add(Lambda(resize_image))\n",
    "\n",
    "    # CNN model\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), strides=(2, 2), padding='same', name='conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', name='conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', name='conv3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(512, name='dense1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(256, name='dense2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(NUM_LABELS, name='output'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed82d01",
   "metadata": {},
   "source": [
    "## Generate Data Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc01f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate data batches\n",
    "def generate_data(samples, labels, batch_size, sample_ratio):\n",
    "    while 1:\n",
    "        for offset in range(0, len(samples), batch_size):\n",
    "            batch_samples = samples[offset:(offset + batch_size)]\n",
    "            batch_labels = labels[offset:(offset + batch_size)]\n",
    "\n",
    "            # Augment each sample in batch\n",
    "            augmented_batch_samples = []\n",
    "            augmented_batch_labels = []\n",
    "            for i in range(len(batch_samples)):\n",
    "                sample = batch_samples[i]\n",
    "                label = batch_labels[i]\n",
    "                augmented_samples, augmented_labels = get_augmented_sample(sample, label, sample_ratio)\n",
    "                augmented_batch_samples.append(augmented_samples)\n",
    "                augmented_batch_labels.append(augmented_labels)\n",
    "\n",
    "            # Flatten out samples and labels\n",
    "            augmented_batch_samples = reduce(operator.add, augmented_batch_samples)\n",
    "            augmented_batch_labels = reduce(operator.add, augmented_batch_labels)\n",
    "\n",
    "            # Reshape input format\n",
    "            X_train = np.array(augmented_batch_samples)\n",
    "            X_train = X_train.reshape(X_train.shape[0], CROP_SIZE, CROP_SIZE, 1)\n",
    "\n",
    "            # Transform input to float and normalize\n",
    "            X_train = X_train.astype('float32')\n",
    "            X_train /= 255\n",
    "\n",
    "            # Encode y\n",
    "            y_train = np.array(augmented_batch_labels)\n",
    "            y_train = to_categorical(y_train, NUM_LABELS)\n",
    "\n",
    "            yield X_train, y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67039c04",
   "metadata": {},
   "source": [
    "## Resize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7eefbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize images\n",
    "def resize_image(img):\n",
    "    size = round(CROP_SIZE / 2)\n",
    "    return tf.image.resize(img, [size, size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899dce1",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2249cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the load_and_preprocess_data() function, add print statements to check data loading and preprocessing.\n",
    "def load_and_preprocess_data():\n",
    "    # Create a dictionary to store each form ID and its writer\n",
    "    form_writer = {}\n",
    "    forms_file_path = \"../data/forms.txt\"\n",
    "\n",
    "    with open(forms_file_path) as f:\n",
    "        for line in islice(f, 16, None):\n",
    "            line_list = line.split(' ')\n",
    "            form_id = line_list[0]\n",
    "            writer = line_list[1]\n",
    "            form_writer[form_id] = writer\n",
    "\n",
    "    # Select the 50 most common writers\n",
    "    num_writers = 50\n",
    "    writers_counter = Counter(form_writer.values())\n",
    "    top_writers = [writer_id for writer_id, _ in writers_counter.most_common(num_writers)]\n",
    "\n",
    "    # Create a temp directory containing only the selected sentences\n",
    "    temp_sentences_path = \"../data/temp_sentences\"\n",
    "    if not os.path.exists(temp_sentences_path):\n",
    "        os.makedirs(temp_sentences_path)\n",
    "\n",
    "    original_sentences_path = os.path.join(\"../data/sentences\", \"*\", \"*\", \"*.png\")\n",
    "\n",
    "    for file_path in glob.glob(original_sentences_path):\n",
    "        image_name = file_path.split(os.path.sep)[-1]\n",
    "        form_id = image_name.split('-')[0] + '-' + image_name.split('-')[1]\n",
    "\n",
    "        if form_writer.get(form_id) in top_writers:  # Use get() to avoid KeyError\n",
    "            try:\n",
    "                shutil.copy(file_path, os.path.join(temp_sentences_path, image_name))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to copy {file_path}. Error: {e}\")\n",
    "\n",
    "    # Create lists of file inputs (a form) and their respective targets (a writer id)\n",
    "    img_files = []\n",
    "    img_targets = []\n",
    "\n",
    "    path_to_files = os.path.join(temp_sentences_path, \"*\", \"*\", \"*.png\")\n",
    "    for file_path in glob.glob(path_to_files):\n",
    "        img_files.append(file_path)\n",
    "        img_targets.append(form_writer[file_path.split(os.path.sep)[-2]])\n",
    "\n",
    "    # Check if there are data points for training\n",
    "    if not img_files:\n",
    "        print(\"No data points available for training. Adjust your criteria or expand the set of writers.\")\n",
    "        #raise ValueError(\"No data points available for training. Adjust your criteria or expand the set of writers.\")\n",
    "    \n",
    "    print(f\"Number of data points for training: {len(img_files)}\")  # Print the number of data points\n",
    "    print(f\"Number of unique writers: {len(set(img_targets))}\")  # Print the number of unique writers\n",
    "    \n",
    "    # Encode target values\n",
    "    encoder = LabelEncoder()\n",
    "    img_targets_encoded = encoder.fit_transform(img_targets)\n",
    "\n",
    "    # Normalize the pixel values and convert images to arrays\n",
    "    img_data = []\n",
    "    for img_path in img_files:\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        img_data.append(np.array(img) / 255.0)\n",
    "\n",
    "    print(f\"Number of images loaded: {len(img_data)}\")  # Print the number of loaded images\n",
    "    assert len(img_data) == len(img_targets_encoded), \"Data and target lengths must be equal\"\n",
    "\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(img_data, img_targets_encoded, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b09d7",
   "metadata": {},
   "source": [
    "## Function to train the writer identification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ca8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_writer_identification_model(X_train, y_train, X_val, y_val):\n",
    "    writer_identification_model = create_writer_identification_model()\n",
    "\n",
    "    # Define model checkpoint to save the best model\n",
    "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    # Train the model\n",
    "    history = writer_identification_model.fit(\n",
    "        generate_data(X_train, y_train, BATCH_SIZE, sample_ratio=0.5),\n",
    "        steps_per_epoch=len(X_train) / BATCH_SIZE,\n",
    "        validation_data=(X_val, to_categorical(y_val, NUM_LABELS)),\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return writer_identification_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7ca94",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ae4497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data points available for training. Adjust your criteria or expand the set of writers.\n",
      "Number of data points for training: 0\n",
      "Number of unique writers: 0\n",
      "Number of images loaded: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     X_train, y_train, X_val, y_val, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Create and train the autoencoder\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     autoencoder \u001b[38;5;241m=\u001b[39m create_autoencoder()\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(img_targets_encoded), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData and target lengths must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Split the dataset into training, validation, and test sets\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_targets_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, y_train, X_val, y_val, X_test, y_test\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2433\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2430\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2432\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2433\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2111\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2108\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2115\u001b[0m     )\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "    # Create and train the autoencoder\n",
    "    autoencoder = create_autoencoder()\n",
    "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Extract features using the autoencoder\n",
    "    encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[3].output)\n",
    "    X_train_features = extract_features(encoder_model, X_train)\n",
    "    X_val_features = extract_features(encoder_model, X_val)\n",
    "    X_test_features = extract_features(encoder_model, X_test)\n",
    "\n",
    "    # Train the writer identification model\n",
    "    writer_identification_model, history = train_writer_identification_model(X_train_features, y_train, X_val_features, y_val)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = writer_identification_model.evaluate(X_test_features, to_categorical(y_test, NUM_LABELS))\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
